\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{main}
\usepackage{times}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage{amsmath}

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{subfig}

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.multipart}
\usepackage{relsize}

\pgfplotsset{
every axis/.append style={
ylabel shift=-1pt,
xlabel shift=-1pt,
xlabel near ticks,
ylabel near ticks,
font=\small
},
compat=1.15
}
\newlength\figureheight
\newlength\figurewidth

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP 2018}
\newcommand\conforg{SIGDAT}

\title{Evaluating the Ability of LSTMs to Learn Context-Free Grammars}

\author{Luzi Sennhauser \\
  Federal Institute of Technology\\
  Zurich, Switzerland \\
  Visiting student at LIDS\\
  Massachusetts Institute of Technology\\
  Cambridge, MA, USA \\
  {\tt luzis@ethz.ch} \\\And
  Robert C. Berwick\\
  LIDS\\ 
  Massachusetts Institute of Technology \\
  Cambridge, MA, USA \\
  {\tt berwick@csail.mit.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

While long short-term memory (LSTM) neural net architectures are designed to capture sequence information, human language is composed of hierarchical (tree) structures. This raises the question as to whether LSTMs can learn hierarchical structures. We explore this question with a well-formed bracket prediction task using two types of brackets modeled by an LSTM.

Demonstrating that such a system is learnable by an LSTM would indicate that the entire class of CFGs is also learnable.  We observe that the model requires exponential memory in terms of the length and depth of training samples, where a sub-linear memory should suffice.

Still, the model does more than simply memorize the training input, and learns how to distinguish between relevant and irrelevant information. On the other hand, we also observe that the model does not generalize well.

We conclude that LSTMs do not learn the relevant underlying context-free rules, suggesting the good overall performance is attained rather by an efficient way of evaluating nuisance variables. LSTMs are a way to quickly reach good results for many natural language tasks, but to understand and generate natural language one has to investigate other concepts that can make more direct use natural language's structural nature.
\end{abstract}

\section{Introduction}

Generative grammars--composing tree structures for natural language--are an extremely powerful tool for human language description.  These structures are of great importance in order to extract sentence semantic interpretation\cite{berwick2016only} and enable us to produce a vast number of sentences with only a very small set of rules. Having acquired such a set of rules, it is easy to construct new structures without having previously seen similar examples.

For purposes of external communication, the syntactic tree structures generated by grammars must be ``flattened'' or linearized into a sequential output form (e.g. written, signed, or spoken). When reading such a (linearized) text or hearing a spoken sentence or observing a signed language, the structure has to be recovered implicitly to recover the original meaning--parsing.

In this study, we investigate whether Long Short-Term Memory (LSTM) models \cite{hochreiter1997long} possess this same feature as humans do: inferring rule-based structure from a linear representation. \citeauthor{everaert2015structures} \shortcite{everaert2015structures} shows clearly that there are phenomena in human language that can only be understood by taking underlying hierarchical structure into account. For neural networks it is therefore essential to learn the underlying structure of sentences.

For tasks like language modeling \cite{mikolov2010recurrent, sundermeyer2012lstm}, parsing \cite{vinyals2015grammar, kiperwasser2016simple, dyer2016recurrent}, machine translation, \cite{bahdanau2014neural} and morphological compositions \cite{kim2016character}, recurrent neural networks are often used. RNNs are inherently sequential models. Since the tree structures appearing in natural language generally correlate with sequential statistical features, it can be difficult to evaluate whether an RNN learns the underlying rules of the sentence's syntax or alternatively simply learns sequential statistical correlations.  In this paper we carry out experiments to determine this.

In the present experiment the LSTM faces a bracket completion problem with two possible bracket types. The optimal solution can only be obtained by inferring the underlying structure. Furthermore every system that can solve this task should be able to recognize every context-free grammar (see section \ref{sec:corpus}).

By analyzing the intermediate states of the network, observing generalization behaviour, and evaluating the memory demand of the model we investigate whether the LSTM learns rules as opposed to statistical regularities.

\section{Related work}
Working with the context-free language $a^nb^n$ and simple context-sensitive languages, \citeauthor{gers2001lstm} \shortcite{gers2001lstm} showed that LSTMs are able to count and partly generalize for the grammars used.  (We note as an aside that in contrast to the language we investigate here, $a^nb^n$ may be considered the ``simplest'' context-free language, since it can be generated by a grammar with just a single nonterminal or bracket type.)
\citeauthor{liska2018memorize} \shortcite{liska2018memorize} investigated the memorization vs. generalization issue for LSTMs for function composition. Since most of the time, it is challenging to figure out what is actually going on with respect to the network's internal state, several attempts have been made to visualize a neural network's intermediate states \cite{rauber2017visualizing, karpathy2015visualizing} with the goal of making them interpretable \cite{krakovna2016increasing}.
Another approach being taken in moving from from sequential network models to structural ones is to hardwire the structural properties into the model's architecture \cite{tai2015improved, kiperwasser2016simple, joulin2015inferring}.
To get a better generalization for rule-based tasks, there are different changes being done to the network architecture. \citeauthor{graves2014neural} \shortcite{graves2014neural} and \citeauthor{sukhbaatar2015end} \shortcite{sukhbaatar2015end} make a larger external memory available to the network while \citeauthor{moshe2017deep} \shortcite{moshe2017deep} make the network architecture dynamic.
% the next paragraph is newly added (partially moved from the abstract)
The question as to whether LSTMs can infer rules was initially explored by others such as \cite{linzen2016assessing} on a natural language corpus, for, e.g., Subject-Verb agreement.

\section{Corpus}
\label{sec:corpus}
When dealing with natural language, there are many side effects  or nuisance variables,  -- e.g. words occurring more often in certain correlative contexts or clusters than others.  These influence any classification and experimental results. To minimize such effects, the following experiments were all conducted on artificial corpora.

Following from the Chomsky-Sch\"utzenberger theorem \cite{chomsky1963algebraic, autebert1997context}, a Dyck language with two types of brackets is the hardest context-free language. Intuitively, every model which recognizes well-formed Dyck words with two types of brackets should be powerful enough to handle any context-free grammar (CFG). (We put to one side here the portion of the theorem that deals with the fact that each language will have a distinct alphabet of terminal and nonterminal types, which is governed by intersection with some regular set.)

The synthetic corpus consists of such a Dyck language with two types of brackets (\verb|[]| and \verb|{}|). Sentences are generated according to the following grammar:

\begin{alltt}
    S  -> S1 S | S1
    S1 -> B | T
    B  -> [ S ] | \{ S \}
    T  -> [ ] | \{ \}
\end{alltt}

The probabilities of the rules are defined in a way that the diversity in terms of distance of a bracket clause and the depth at which a bracket appears is large. All 1M generated sentences have a length of 100 characters.

In this paper, we check if an LSTM can be trained to recognize this grammar.

\setlength\figureheight{4cm}
\setlength\figurewidth{\linewidth}
\begin{figure}[ht]
    \input{figures/corpus_distance_frequencies.tex}\\%
    \input{figures/corpus_depth_frequencies.tex}%
    \caption{Corpus frequencies}%
    \label{fig:corpus_frequencies}%
\end{figure}

\section{Model}

To check if we can train a neural network to accept the language generated by the grammar above, an LSTM is used.

\subsection{Long Short-Term Memory}

Long-Short-Term-Memory networks (LSTM) \cite{hochreiter1997long} are a variant of recurrent neural networks (RNNs). Both of them possess a memory state that is updated in the process of reading a time series. Many RNNs suffer from the problem of vanishing gradients \cite{hochreiter1997long}. Often-used recurrent activation functions of RNNs are $tanh$ or the sigmoid function. Since the gradient of $tanh$ is upper bounded by $0.5$, and of the sigmoid function even by $0.25$, the gradient cannot be conserved during backpropagation and vanishes. LSTMs deal with this issue by containing three multiplicative gates controlling what proportion of the input to pass to the memory cell (input gate), what proportion of the previous memory cell information to discard (forget gate) and what proportion of the memory cell to output (output gate). In the recurrency of the LSTM the activation function is the identity function, which has gradient $1.0$. This means that if the forget gate is open, the gradient is fully passed on to previous time steps, and long term dependencies can be learned.

The LSTM reads each input $x_i$ consecutively and updates its memory state $c_i$ accordingly. After each step, an output $h_i$ is generated based on the updated memory state. More specifically, the LSTM solves the following equations in a forward pass:
%
\begin{equation}
\begin{split}
    \mathbf{i}_t &= \sigma(\mathbf{W}_{ix}\mathbf{x}_t+\mathbf{W}_{ih}\mathbf{h}_{t-1}+\mathbf{b}_i) \\
    \mathbf{f}_t &= \sigma(\mathbf{W}_{fx}\mathbf{x}_t+\mathbf{W}_{fh}\mathbf{h}_{t-1}+\mathbf{b}_f) \\
    \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1}\\
    & \qquad +\mathbf{i}_t \odot tanh(\mathbf{W}_{cx}\mathbf{x}_t + \mathbf{W}_{ch}\mathbf{h}_{t-1}+\mathbf{b}_c) \\
    \mathbf{o}_t &= \sigma(\mathbf{W}_{ox}\mathbf{x}_t + \mathbf{W}_{oh}\mathbf{h}_{t-1} + \mathbf{b}_o)\\
    \mathbf{h}_t &= \mathbf{o}_t \odot tanh(\mathbf{c}_t) 
    \label{eq:lstm_equations}
\end{split}
\end{equation}

\begin{figure}[h]
    \centering
    \input{figures/lstm_cell.tex}
    \caption{schematic model of an LSTM-cell. $\bigodot$ stands for element-wise multiplication and $\bigoplus$ for vector addition.}
    \label{fig:lstm_cell}

\end{figure}
%
\subsection{Basic Model}

Now let us turn to the details of the model implementation.  We begin with the basic formulation.

Let $B_{open}$ and $B_{close}$ be the sets of opening and closing brackets and $B = B_{open} \cup B_{close}$ the set of all brackets. Given the beginning of a sentence $w_1, w_2, ..., w_{i-1}, w_i$ with $w_1, ..., w_{i-1} \in B$ and $w_i \in B_{close}$, the LSTMs tries approximate the function:
%
\begin{align*}
  F \colon B^{i-1} &\to B_{close}\\
  w_1, w_2, ..., w_{i-1} &\mapsto w_i.
  \label{eq:brackets_task_definition}
\end{align*}
%
The clause between the corresponding opening bracket of $w_i$ and $w_i$ will be referred to as the \emph{relevant clause} in the course of this paper. Likewise with \emph{distance} we denote the length of the relevant clause. Note that since a bracket pair consists of 2 symbols, distances are always even. We denote \emph{odd distances} as ${2,6,10,14,...}$ and \emph{even distances} as ${4,8,12,16,...}$.

To read the input symbols, an embedding layer with 5 output dimensions precedes the LSTM. Together they build the encoder, which will read the input sequentially. The decoder, mapping the internal representation to a probability of predicting \verb|}| or \verb|]| is a dense layer with one output variable.

\begin{figure*}[ht]
    \centering
    \input{figures/lstm_architecture.tex}
    \caption{Network architecture of the model. The basic end-to-end model consists of the encoder and the basic decoder. The analysis model fixes the weights for the encoder and takes the scalar or sequence analysis decoder depending on the dimension of $z$.}
    \label{fig:lstm_architecture}
\end{figure*}

It turns out that the initialization of the model is crucial to avoid bad local minima. We have compared different initialization methods. The following results in consistently good solutions: To initialize the weights, the model is trained with sentences of length of 50 and only afterwards on the actual corpus with sentence length 100.

For backpropagation, the Adam \cite{kingma2014adam} optimizer was used. Furthermore to ensure faster and more consistent convergence, at the beginning of the training, the batch size is gradually increased, which has a similar effect as reducing the learning rate \cite{smith2017don}.

\subsection{Analysis Model}

To analyze the internal representation $[h_i, c_i]$ of the LSTM after having read the input, we use a method already used by \citeauthor{shi2016does} \shortcite{shi2016does} and \citeauthor{belinkov2017neural} \shortcite{belinkov2017neural}. After having trained the basic model, the weights of the encoder are fixed and the labels (previously $y$) are changed to some feature $z_i$ of the input $x_1,\ldots, x_i$.

This feature $z_i$ can either be a scalar or a vector. If $z_i$ is a scalar, a dense layer (scalar analysis decoder) is trained to predict $z_i$. On the other hand, if $z_i$ is a vector, another LSTM is trained to predict $z_{i,1},\ldots,z_{i,j}$.\\

Analyzing the performance of the analysis network shows us how accurate a feature $z_i$ is preserved in $[h_i, c_i]$. One can assume that the LSTM uses its limited memory ``efficiently'' and therefore discards irrelevant information. Hence the performance of the analysis decoder shows whether $z_i$ is contained in the information that is relevant for the original classification task.

To begin, two of the experiments which were conducted are presented in the following section to test the trained model performance. For the first experiment $z_i$ is the depth (nesting level) after having read $i$ characters.

\textbf{Example:} For the sequence \verb|{[{}[[]|, it holds that $z = (1,2,3,2,3,4,3)$.\\

Theoretically, at any time $t$, no information about a closed clause in $w_1,\ldots, w_t$ has to be stored, since it is irrelevant for any eventual future prediction of $w_{t+1}, w_{t+2},\ldots$. 
When reading from left to right, as soon as a closing bracket is processed, the corresponding clause becomes irrelevant. Therefore, the relevant information is simply the list of bracket types of unclosed clauses. In this experiment we investigate how well the previous symbols are preserved in the intermediate representation and evaluate if this correlates the recovered symbols being relevant or not.

\textbf{Example:} after having processed \verb|{[{}][|, only the first and the last characters are relevant, since they are the only ones that could matter for a future classification task. On the other hand, the sub-string \verb|[{}]| is irrelevant. In this example we would evaluate whether the first and last character are better preserved in the intermediate state than the irrelevant sub-string.

To set up the experiment, we set $z_{i,k}$ to be equal to $x_{i-k+1}$, corresponding to predicting the previous symbols of a given intermediate state.

\subsection{Varying hidden units}
\label{subsec:varying_hidden_units}

The basic model is evaluated with $2,4,6,...,50$ hidden units. The error rate with 50 hidden units is 0.38\% and an error rate of 1\% is reached around 20 hidden units.  Thus, the error seems to converge with increasing hidden units to a fairly small value. As a result, in all further experiments, the maximum number of hidden units the models are tested against was set to 50.

\setlength\figureheight{4cm}
\setlength\figurewidth{\linewidth}
\begin{figure}[ht]
    \input{figures/varying_units_results.tex}%
    \caption{Error rate with respect to the number of hidden units of the LSTM.}%
    \label{fig:varying_units_results}%
\end{figure}

\subsection{Memory demand}
\label{subsec:memory_demand}

In this section we evaluate how ``difficult'' sentences can be with respect to the memory demand of the model, while still reaching an error tolerance of 5\%. We have to work with tolerances, because 100\% accuracy is not reached. Since it can be challenging to measure how difficult a sentence is to predict, we use the distance of a sentence as defined above and the depth at which the relevant clause occurs as metrics.

\setlength\figureheight{5cm}
\setlength\figurewidth{0.48\textwidth}
\begin{figure*}[ht]
    \input{figures/memory_demand/distance.tex}\qquad
    \input{figures/memory_demand/depth_increase.tex}
    \caption{Distances (left figure) and embedded depth (right figure) that can be predicted with a given number of hidden units and 5\% error tolerance. The dashed line is a logarithmic approximation.}%
    \label{fig:memory_demand}%
\end{figure*}

The resulting values (figure \ref{fig:memory_demand}) demonstrate that memory demand grows exponentially with respect to the distance of sentences that can be predicted. The same behaviour can be observed with respect to an increase in depth. 

\subsection{Generalization}
\label{subsec:generalization}

To evaluate the model's generalization performance, training was done only on a systematically chosen subset of sentences.

The results (figure \ref{fig:generalization_results}) demonstrate a large discrepancy between in-sample and the out-of-sample accuracy. The experiment was evaluated for different numbers of hidden units. On the one hand, with a large number of hidden units, the generalization error is similarly large (the out-of-sample error rate was already for interpolation between 8.1 and 14.3 times larger than the in-sample error).

On the other hand, models with a small number of hidden units did not even converge. The reason for no convergence can be reasonably be explained by the sparse data set, that might lead to more local minima. The maximum generality -- especially for smaller distances -- is observed at around 10 hidden units.

Generalization has been evaluated with respect to distance and with respect to the embedded depth (maximum nesting level of the relevant clause). For each of these metrics, generalization has been checked for interpolation and extrapolation.

For interpolation, training is done on odd distances / embedding depths (in-sample). To check for extrapolation for distances, the model is trained with sentences of distance / embedding depth smaller than 12. For testing, all remaining sentences are used.

For interpolation the out-of-sample error for 10 hidden units was on average 5.4 (distance) and 5.9 (embedded depth) higher than the in-sample error. Figure \ref{fig:generalization_results} shows also that for extrapolation, the model generalizes much worse or not at all.

\setlength\figureheight{4cm}
\setlength\figurewidth{0.45\textwidth}
\begin{figure*}[ht]
    \bgroup
    \def\arraystretch{1.5}%
    \begin{tabular}{c|c c}
        & distance & embedded depth\\
        \hline
        \raisebox{0.45in}{\rotatebox{90}{interpolation}} &
        \input{figures/generalization/distance_even_odd.tex} & \input{figures/generalization/depth_increase_even_odd.tex}\\
        \raisebox{0.45in}{\rotatebox{90}{extrapolation}} & \input{figures/generalization/distance_extrapolation.tex} &
        \input{figures/generalization/depth_increase_extrapolate.tex}
    \end{tabular}
    \egroup
    \caption{Test for generalization: The error rate of the model with 10 hidden units if only half of the corpus is systematically selected for training (in-sample), while during testing also the left out distances were considered (out-of-sample).}%
    \label{fig:generalization_results}%
\end{figure*}

\subsection{Intermediate State Analysis}
\label{subsec:intermediate_state_analysis}

For the first experiment with intermediate states, recovering the depth from intermediate states (figure \ref{fig:analysis_depth_results}, shows that the depth is only marginally conserved in the intermediate state. For a small number of units, the model is able to distinguish whether a depth is either close to $0$ or close to the mean depth. A nice example for this behaviour can be seen in figure \ref{fig:analysis_depth_results} for two hidden units. When increasing the number of hidden units, the distribution of predictions gets closer to the real distribution of depths, while for two hidden units, the prediction of the depth is on average off by $7.04$ and decreases until it reaches a value of $1.34$ for 50 hidden units.

\setlength\figureheight{6cm}
\setlength\figurewidth{1.05\linewidth}
\begin{figure}[ht]
    \input{figures/analysis_depth_results.tex}%
    \caption{Error rate of predicting the depth given an intermediate state of the basic model. It is evaluated for $2,8,50$ hidden units. The dashed line indicates the baseline predicting the mean depth. The error indicates how far the prediction is off from the true depth.}%
    \label{fig:analysis_depth_results}%
\end{figure}

Figure \ref{fig:analysis_previous_results} shows how accurate a past symbol can be recovered after being given an intermediate state. There is a large discrepancy between the accuracy of relevant and irrelevant symbols. If the 4th-to-last symbol is an irrelevant one, the model is only able to recover the type of bracket with a 33\% error rate; whereas if it is a relevant symbol, it reaches an error below 1.8\%. As the number of past symbols $k$ approaches 10, the irrelevant information cannot be recovered any more. Note that an error rate of $0.5$ amounts to a random guess, since we evaluate only if it can predict the type of bracket (square or curly) correctly, and not whether it was an opening or a closing one.

\setlength\figureheight{4cm}
\setlength\figurewidth{\linewidth}
\begin{figure}[ht]
    \input{figures/analysis_previous_results.tex}%
    \caption{Error rate of predicting the previous symbols given an intermediate state of the basic model with 20 hidden units. Characters are grouped as being either relevant or irrelevant for the basic classification task.}%
    \label{fig:analysis_previous_results}%
\end{figure}

\section{Discussion}

We now consider the results of the various experiments, some of which might be considered as controversial at first sight. On the one hand, we see that the LSTM exhibits an exponential memory demand as sentences grow longer, while theoretically, a sub-linear memory would suffice \cite{magniez2014recognizing}. On the other hand, we see that the model has successfully sorted out irrelevant information: the intermediate state analysis shows that irrelevant characters are very quickly forgotten. So. the exponential memory space is not needed for storing irrelevant information for the original classification task.

The strength of structural rules is that they generalize well. In human language this enables humans to create new sentences which have never been heard before. But also for the Dyck language being used, the 4 rules defining the language are enough to generate sentences of arbitrary distance and depth. The only constraint is the memory to store intermediate results while streaming the input. Assuming the model had in fact learned the underlying grammatical rules correctly, an upper bound for the memory required is 50 bits. The model we are using has up to 50 hidden units which corresponds to 11,200 trainable parameters. %

\citeauthor{collins2016capacity} \shortcite{collins2016capacity} showed that LSTMs can store up to 5 bits of information per parameter and one real number per hidden unit. 
What we observe then is that the LSTM generalizes poorly. Given that memory capacity cannot be an issue, we conclude that the model is not able to learn the right rules.

Combining the generalization results and the intermediate state analysis reveals that the model determines the each characters' relevance -- but it has learned this without resorting to hierarchical rules. As LSTMs are known to have the ability to capture statistical contingencies, it suggests instead that rather than the ``perfect'' rule-based solution, what the LSTM has in fact acquired is a sequential statistical approximation to this solution.

The large effect of initialization to a good local minimum suggests that the underlying function may well have many local minima.Indeed, \citeauthor{collins2016capacity} \shortcite{collins2016capacity} has already concluded, that the memory in LSTMs is mainly used for training effectiveness rather than to increase the storage capacity. Therefore, the large memory demand in our experiments suggests that the LSTM memory is needed to avoid such local minima.

\section{Conclusion}

At heart, neural networks are statistical models, performing well at capturing and combining correlations of the output variable values and the corresponding component values in the training input. In particular, LSTMs are constructed such that they capture sequential information. Hence, due to the design of their architecture, LSTMs perform very well on statistically-oriented, sequential tasks. 

As a result, in experiments like this one that examine whether LSTMs can acquire hierarchical knowledge, one has to pay close attention to nuisance variables like sequential statistical correlations that might be hard to detect and confounded with true hierarchical information.

The bottom line that emerges from this experiment is that the range of rules that an LSTM can learn is very restricted: even a context-free grammar with four simple rules, cannot be appropriately learned by an LSTM. 

According to most linguistic accounts, natural language syntax relies heavily on hierarchical rules. It enables humans to compose new sentences with relatively little memory capacity and training data. Furthermore there are sentences that have the same linear representation but differ in structure-- syntactically ambiguous sentences.  From this perspective, it seems not only more efficient to directly infer structures and rules, but also useful to use rules to understand sentences correctly. The bracket completion task presented here can be understood by a human after only a few training sentences, though online processing of the rules themselves may be difficult. This result invites the conclusion that it will be very challenging for LSTMs to understand natural language as humans do. While LSTMs remain good engineering tools to approximate certain language features based on statistical correlations, the exploration of fundamentally new models and architectures seems a valuable direction to explore on the way to developing methods for understanding human language in the way that people do.

\section*{Acknowledgments}
I want to thank Prof. Robert C. Berwick and Beracah Yankama for all their help, valuable discussions and the machines to run the experiments on. I thank Prof. Thomas Hofmann for the fast and easy administrative process at ETH Zurich and also for granting access to high-computing clusters. Additionally I am very grateful for the financial support provided by the Zeno Karl Schindler Foundation.

\bibliography{main}
\bibliographystyle{acl_natbib_nourl}

\end{document}
